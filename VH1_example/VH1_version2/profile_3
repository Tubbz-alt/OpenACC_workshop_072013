CrayPat/X:  Version 5.3.1 Revision 8907 (xf 8679)  03/08/12 12:37:40

Number of PEs (MPI ranks):   16
                           
Numbers of PEs per Node:     16
                           
Numbers of Threads per PE:    1
                           
Number of Cores per Socket:  16

Execution start time:  Wed Mar 21 10:22:33 2012

System type and speed:  x86_64 2200 MHz

Current path to data file:
  /ccs/home/levesque/lustre/VH1_version1/vhone+pat+7694-11446t.ap2  (RTS)



Notes for table 1:

  Table option:
    -O profile
  Options implied by table option:
    -d ti%@0.95,ti,imb_ti,imb_ti%,tr -b gr,fu,pe=HIDE
  Other options:
    -T 

  Options for related tables:
    -O profile_pe.th           -O profile_th_pe       
    -O profile+src             -O load_balance        
    -O callers                 -O callers+src         
    -O calltree                -O calltree+src        

  The Total value for Time, Calls is the sum for the Group values.
  The Group value for Time, Calls is the sum for the Function values.
  The Function value for Time, Calls is the avg for the PE values.
    (To specify different aggregations, see: pat_help report options s1)

  This table shows only lines with Time% > 0.

  Percentages at each level are of the Total for the program.
    (For percentages relative to next level up, specify:
      -s percent=r[elative])


Table 1:  Profile by Function Group and Function

 Time%  |     Time  |    Imb.  |  Imb.  | Calls  |Group 
        |           |    Time  | Time%  |        | Function 
        |           |          |        |        |  PE=HIDE 
       
 100.0% | 22.720304 |       -- |     -- | 1476.2 |Total
|-----------------------------------------------------------------------
|  66.3% | 15.073981 |       -- |     -- |  377.0 |USER
||----------------------------------------------------------------------
||  20.1% |  4.556077 | 0.390793 |   8.4% |  150.0 |#3.sweepy
||  19.6% |  4.457468 | 0.395731 |   8.7% |   75.0 |#4.sweepz
||   9.7% |  2.209909 | 0.199750 |   8.8% |   75.0 |#2.sweepx2
||   9.5% |  2.165342 | 0.152996 |   7.0% |   75.0 |#1.sweepx1
||   7.4% |  1.685158 | 5.368438 |  81.2% |    1.0 |vhone_
||   0.0% |  0.000026 | 0.000024 |  50.3% |    1.0 |exit
||======================================================================
|  30.2% |  6.868182 |       -- |     -- |  544.5 |MPI_SYNC
||----------------------------------------------------------------------
||  28.4% |  6.451346 | 5.898978 |  91.4% |  450.0 |mpi_alltoall_(sync)
||   1.4% |  0.321020 | 0.316969 |  98.7% |   76.0 |mpi_allreduce_(sync)
||   0.4% |  0.095816 | 0.095198 |  99.4% |   18.5 |mpi_gather_(sync)
||======================================================================
|   3.4% |  0.778141 |       -- |     -- |  554.7 |MPI
||----------------------------------------------------------------------
||   2.8% |  0.646661 | 0.121935 |  16.9% |  450.0 |mpi_alltoall
||   0.4% |  0.090727 | 0.059984 |  42.5% |    1.0 |mpi_finalize
||   0.2% |  0.038273 | 0.009699 |  21.6% |   18.5 |mpi_gather
||   0.0% |  0.001423 | 0.000146 |   9.9% |   76.0 |MPI_ALLREDUCE
||   0.0% |  0.001032 | 0.000751 |  44.9% |    2.0 |mpi_comm_split
||   0.0% |  0.000012 | 0.000002 |  16.3% |    3.0 |MPI_COMM_SIZE
||   0.0% |  0.000008 | 0.000004 |  32.8% |    3.0 |mpi_comm_rank
||   0.0% |  0.000002 | 0.000001 |  42.6% |    1.0 |MPI_INIT
||   0.0% |  0.000002 | 0.000023 | 100.0% |    0.1 |mpi_wtime
||   0.0% |  0.000001 | 0.000015 | 100.0% |    0.1 |MPI_GET_PROCESSOR_NAME
|=======================================================================


Notes for table 2:

  Table option:
    -O profile+hwpc
  Options implied by table option:
    -d ti%@0.95,ti,imb_ti,imb_ti%,tr,P -b gr,fu,pe=HIDE
  Other options:
    -T 

  Options for related tables:
    -O profile_pe.th           -O profile_th_pe       
    -O profile+src             -O load_balance        
    -O callers                 -O callers+src         
    -O calltree                -O calltree+src        

  The Total value for each data item is the sum for the Group values.
  The Group value for each data item is the sum for the Function values.
  The Function value for each data item is the avg for the PE values.
    (To specify different aggregations, see: pat_help report options s1)

  This table shows only lines with Time% > 0.

  Percentages at each level are of the Total for the program.
    (For percentages relative to next level up, specify:
      -s percent=r[elative])


Table 2:  Profile by Function Group and Function

Group / Function / PE=HIDE

  
=====================================================================
  Total
---------------------------------------------------------------------
  Time%                                    100.0% 
  Time                                  22.720304 secs
  Imb. Time                                    -- secs
  Imb. Time%                                   -- 
  Calls                  64.972 /sec       1476.2 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3      420.142M/sec   9545736941 ops
  Average Time per Call                  0.015391 secs
  CrayPat Overhead : Time  0.0%                   
  User time (approx)     22.720 secs  49984803035 cycles  100.0% Time
=====================================================================
  USER
---------------------------------------------------------------------
  Time%                                     66.3% 
  Time                                  15.073981 secs
  Imb. Time                                    -- secs
  Imb. Time%                                   -- 
  Calls                  25.014 /sec        377.0 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3      633.366M/sec   9545733356 ops
  Average Time per Call                  0.039984 secs
  CrayPat Overhead : Time  0.0%                   
  User time (approx)     15.071 secs  33157296871 cycles  100.0% Time
=====================================================================
  USER / #3.sweepy
---------------------------------------------------------------------
  Time%                                     20.1% 
  Time                                   4.556077 secs
  Imb. Time                              0.390793 secs
  Imb. Time%                                 8.4% 
  Calls                  32.920 /sec        150.0 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3      695.681M/sec   3169881480 ops
  Average Time per Call                  0.030374 secs
  CrayPat Overhead : Time  0.0%                   
  User time (approx)      4.557 secs  10024371436 cycles  100.0% Time
=====================================================================
  USER / #4.sweepz
---------------------------------------------------------------------
  Time%                                     19.6% 
  Time                                   4.457468 secs
  Imb. Time                              0.395731 secs
  Imb. Time%                                 8.7% 
  Calls                  16.825 /sec         75.0 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3      713.726M/sec   3181569780 ops
  Average Time per Call                  0.059433 secs
  CrayPat Overhead : Time  0.0%                   
  User time (approx)      4.458 secs   9806948915 cycles  100.0% Time
=====================================================================
  USER / #2.sweepx2
---------------------------------------------------------------------
  Time%                                      9.7% 
  Time                                   2.209909 secs
  Imb. Time                              0.199750 secs
  Imb. Time%                                 8.8% 
  Calls                  33.935 /sec         75.0 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3      718.972M/sec   1589018579 ops
  Average Time per Call                  0.029465 secs
  CrayPat Overhead : Time  0.0%                   
  User time (approx)      2.210 secs   4862296267 cycles  100.0% Time
=====================================================================
  USER / #1.sweepx1
---------------------------------------------------------------------
  Time%                                      9.5% 
  Time                                   2.165342 secs
  Imb. Time                              0.152996 secs
  Imb. Time%                                 7.0% 
  Calls                  34.632 /sec         75.0 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3      734.140M/sec   1589860322 ops
  Average Time per Call                  0.028871 secs
  CrayPat Overhead : Time  0.0%                   
  User time (approx)      2.166 secs   4764355203 cycles  100.0% Time
=====================================================================
  USER / vhone_
---------------------------------------------------------------------
  Time%                                      7.4% 
  Time                                   1.685158 secs
  Imb. Time                              5.368438 secs
  Imb. Time%                                81.2% 
  Calls                   0.595 /sec          1.0 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3        9.160M/sec     15403186 ops
  Average Time per Call                  1.685158 secs
  CrayPat Overhead : Time  0.0%                   
  User time (approx)      1.681 secs   3699267169 cycles  100.0% Time
=====================================================================
  USER / exit
---------------------------------------------------------------------
  Time%                                      0.0% 
  Time                                   0.000026 secs
  Imb. Time                              0.000024 secs
  Imb. Time%                                50.3% 
  Calls                   0.038M/sec          1.0 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3        0.359M/sec        9.438 ops
  Average Time per Call                  0.000026 secs
  CrayPat Overhead : Time  5.4%                   
  User time (approx)      0.000 secs        57882 cycles  100.0% Time
=====================================================================
  MPI_SYNC
---------------------------------------------------------------------
  Time%                                     30.2% 
  Time                                   6.868182 secs
  Imb. Time                                    -- secs
  Imb. Time%                                   -- 
  Calls                  79.285 /sec        544.5 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3                              0 ops
  Average Time per Call                  0.012614 secs
  CrayPat Overhead : Time  0.0%                   
  User time (approx)      6.868 secs  15108820589 cycles  100.0% Time
=====================================================================
  MPI_SYNC / mpi_alltoall_(sync)
---------------------------------------------------------------------
  Time%                                     28.4% 
  Time                                   6.451346 secs
  Imb. Time                              5.898978 secs
  Imb. Time%                                91.4% 
  Calls                  69.758 /sec        450.0 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3                              0 ops
  Average Time per Call                  0.014336 secs
  CrayPat Overhead : Time  0.0%                   
  User time (approx)      6.451 secs  14191914307 cycles  100.0% Time
=====================================================================
  MPI_SYNC / mpi_allreduce_(sync)
---------------------------------------------------------------------
  Time%                                      1.4% 
  Time                                   0.321020 secs
  Imb. Time                              0.316969 secs
  Imb. Time%                                98.7% 
  Calls                 236.743 /sec         76.0 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3                              0 ops
  Average Time per Call                  0.004224 secs
  CrayPat Overhead : Time  0.0%                   
  User time (approx)      0.321 secs    706252250 cycles  100.0% Time
=====================================================================
  MPI_SYNC / mpi_gather_(sync)
---------------------------------------------------------------------
  Time%                                      0.4% 
  Time                                   0.095816 secs
  Imb. Time                              0.095198 secs
  Imb. Time%                                99.4% 
  Calls                 193.208 /sec         18.5 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3                              0 ops
  Average Time per Call                  0.005179 secs
  CrayPat Overhead : Time  0.0%                   
  User time (approx)      0.096 secs    210654033 cycles  100.0% Time
=====================================================================
  MPI
---------------------------------------------------------------------
  Time%                                      3.4% 
  Time                                   0.778141 secs
  Imb. Time                                    -- secs
  Imb. Time%                                   -- 
  Calls                 710.029 /sec        554.7 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3        0.005M/sec         3585 ops
  Average Time per Call                  0.001403 secs
  CrayPat Overhead : Time  0.1%                   
  User time (approx)      0.781 secs   1718685574 cycles  100.0% Time
=====================================================================
  MPI / mpi_alltoall
---------------------------------------------------------------------
  Time%                                      2.8% 
  Time                                   0.646661 secs
  Imb. Time                              0.121935 secs
  Imb. Time%                                16.9% 
  Calls                 692.673 /sec        450.0 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3        0.005M/sec         3357 ops
  Average Time per Call                  0.001437 secs
  CrayPat Overhead : Time  0.1%                   
  User time (approx)      0.650 secs   1429250822 cycles  100.0% Time
=====================================================================
  MPI / mpi_finalize
---------------------------------------------------------------------
  Time%                                      0.4% 
  Time                                   0.090727 secs
  Imb. Time                              0.059984 secs
  Imb. Time%                                42.5% 
  Calls                  11.022 /sec          1.0 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3        2.067 /sec        0.188 ops
  Average Time per Call                  0.090727 secs
  CrayPat Overhead : Time  0.0%                   
  User time (approx)      0.091 secs    199597051 cycles  100.0% Time
=====================================================================
  MPI / mpi_gather
---------------------------------------------------------------------
  Time%                                      0.2% 
  Time                                   0.038273 secs
  Imb. Time                              0.009699 secs
  Imb. Time%                                21.6% 
  Calls                 482.514 /sec         18.5 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3        0.002M/sec       59.500 ops
  Average Time per Call                  0.002069 secs
  CrayPat Overhead : Time  0.1%                   
  User time (approx)      0.038 secs     84350214 cycles  100.0% Time
=====================================================================
  MPI / MPI_ALLREDUCE
---------------------------------------------------------------------
  Time%                                      0.0% 
  Time                                   0.001423 secs
  Imb. Time                              0.000146 secs
  Imb. Time%                                 9.9% 
  Calls                   0.053M/sec         76.0 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3        0.110M/sec          158 ops
  Average Time per Call                  0.000019 secs
  CrayPat Overhead : Time  7.6%                   
  User time (approx)      0.001 secs      3163798 cycles  100.0% Time
=====================================================================
  MPI / mpi_comm_split
---------------------------------------------------------------------
  Time%                                      0.0% 
  Time                                   0.001032 secs
  Imb. Time                              0.000751 secs
  Imb. Time%                                44.9% 
  Calls                   0.002M/sec          2.0 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3        0.009M/sec        9.625 ops
  Average Time per Call                  0.000516 secs
  CrayPat Overhead : Time  0.3%                   
  User time (approx)      0.001 secs      2272414 cycles  100.0% Time
======================================================================
  MPI / MPI_COMM_SIZE
----------------------------------------------------------------------
  Time%                                       0.0% 
  Time                                    0.000012 secs
  Imb. Time                               0.000002 secs
  Imb. Time%                                 16.3% 
  Calls                    0.253M/sec          3.0 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3                               0 ops
  Average Time per Call                   0.000004 secs
  CrayPat Overhead : Time  35.5%                   
  User time (approx)       0.000 secs        26052 cycles  100.0% Time
======================================================================
  MPI / mpi_comm_rank
----------------------------------------------------------------------
  Time%                                       0.0% 
  Time                                    0.000008 secs
  Imb. Time                               0.000004 secs
  Imb. Time%                                 32.8% 
  Calls                    0.372M/sec          3.0 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3                               0 ops
  Average Time per Call                   0.000003 secs
  CrayPat Overhead : Time  52.9%                   
  User time (approx)       0.000 secs        17734 cycles  100.0% Time
======================================================================
  MPI / MPI_INIT
----------------------------------------------------------------------
  Time%                                       0.0% 
  Time                                    0.000002 secs
  Imb. Time                               0.000001 secs
  Imb. Time%                                 42.6% 
  Calls                    0.644M/sec          1.0 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3                               0 ops
  Average Time per Call                   0.000002 secs
  CrayPat Overhead : Time  89.1%                   
  User time (approx)       0.000 secs         3416 cycles  100.0% Time
======================================================================
  MPI / mpi_wtime
----------------------------------------------------------------------
  Time%                                       0.0% 
  Time                                    0.000002 secs
  Imb. Time                               0.000023 secs
  Imb. Time%                                100.0% 
  Calls                    0.122M/sec          0.1 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3         0.243M/sec        0.250 ops
  Average Time per Call                   0.000012 secs
  CrayPat Overhead : Time  11.4%                   
  User time (approx)       0.000 secs         2263 cycles  100.0% Time
======================================================================
  MPI / MPI_GET_PROCESSOR_NAME
----------------------------------------------------------------------
  Time%                                       0.0% 
  Time                                    0.000001 secs
  Imb. Time                               0.000015 secs
  Imb. Time%                                100.0% 
  Calls                    0.076M/sec          0.1 calls
  DISPATCHED_FPU_OPS:
    OPS_DUAL_PIPE0:
    OPS_DUAL_PIPE1:
    OPS_DUAL_PIPE2:
    OPS_DUAL_PIPE3         0.152M/sec        0.125 ops
  Average Time per Call                   0.000016 secs
  CrayPat Overhead : Time   8.7%                   
  User time (approx)       0.000 secs         1812 cycles  100.0% Time


Notes for table 3:

  Table option:
    -O load_balance_m
  Options implied by table option:
    -d ti%@0.95,ti,Mc,Mm,Mz -b gr,pe=[mmm]
  Other options:
    -T 

  Options for related tables:
    -O load_balance_sm         -O load_balance_cm     

  The Total value for each data item is the sum for the Group values.
  The Group value for each data item is the avg for the PE values.
    (To specify different aggregations, see: pat_help report options s1)

  This table shows only lines with Time% > 0.

  Percentages at each level are of the Total for the program.
    (For percentages relative to next level up, specify:
      -s percent=r[elative])


Table 3:  Load Balance with MPI Message Stats

 Time%  |     Time  |   MPI  |    MPI Msg  |   Avg MPI  |Group 
        |           |   Msg  |      Bytes  |  Msg Size  | PE=[mmm] 
        |           | Count  |             |            |
       
 100.0% | 22.722400 |  544.5 | 688825304.0 | 1265060.25 |Total
|------------------------------------------------------------------
|  66.3% | 15.074517 |    0.0 |         0.0 |         -- |USER
||-----------------------------------------------------------------
||  94.8% | 21.541182 |    0.0 |         0.0 |         -- |pe.0
||  59.3% | 13.474350 |    0.0 |         0.0 |         -- |pe.9
||  57.9% | 13.157368 |    0.0 |         0.0 |         -- |pe.14
||=================================================================
|  30.2% |  6.868955 |    0.0 |         0.0 |         -- |MPI_SYNC
||-----------------------------------------------------------------
||  38.4% |  8.728522 |    0.0 |         0.0 |         -- |pe.15
||  36.8% |  8.360156 |    0.0 |         0.0 |         -- |pe.2
||   2.5% |  0.557842 |    0.0 |         0.0 |         -- |pe.0
||=================================================================
|   3.4% |  0.778928 |  544.5 | 688825304.0 | 1265060.25 |MPI
||-----------------------------------------------------------------
||   4.1% |  0.937464 |  537.0 | 688750304.0 | 1282589.02 |pe.9
||   3.3% |  0.753740 |  552.0 | 688900304.0 | 1248007.80 |pe.8
||   2.7% |  0.623284 |  567.0 | 689050304.0 | 1215256.27 |pe.0
|==================================================================


Notes for table 4:

  Table option:
    -O mpi_callers
  Options implied by table option:
    -d Mm,Mc@,Mb1..7 -b fu,ca,pe=[mmm]
  Other options:
    -T 

  Options for related tables:
    -O mpi_sm_callers          -O mpi_coll_callers    

  The Total value for each data item is the sum for the Function values.
  The Function value for each data item is the sum for the Caller values.
  The Caller value for each data item is the avg for the PE values.
    (To specify different aggregations, see: pat_help report options s1)

  This table shows only lines with MPI Msg Count > 0.


Table 4:  MPI Message Stats by Caller

    MPI Msg  |   MPI  | MsgSz  | 4KB<=  | 64KB<=  | 1MB<=  |Function 
      Bytes  |   Msg  |  <16B  | MsgSz  |  MsgSz  | MsgSz  | Caller 
             | Count  | Count  | <64KB  |   <1MB  | <16MB  |  PE=[mmm] 
             |        |        | Count  |  Count  | Count  |
            
 688825304.0 |  544.5 |   76.0 |    7.5 |   450.0 |   11.0 |Total
|------------------------------------------------------------------------
| 675000000.0 |  450.0 |    0.0 |    0.0 |   450.0 |    0.0 |mpi_alltoall
||-----------------------------------------------------------------------
|| 450000000.0 |  300.0 |    0.0 |    0.0 |   300.0 |    0.0 |sweepy_
3|             |        |        |        |         |        | vhone_
||||---------------------------------------------------------------------
4||| 450000000.0 |  300.0 |    0.0 |    0.0 |   300.0 |    0.0 |pe.0
4||| 450000000.0 |  300.0 |    0.0 |    0.0 |   300.0 |    0.0 |pe.8
4||| 450000000.0 |  300.0 |    0.0 |    0.0 |   300.0 |    0.0 |pe.15
||||=====================================================================
|| 112500000.0 |   75.0 |    0.0 |    0.0 |    75.0 |    0.0 |sweepz_
3|             |        |        |        |         |        | vhone_
||||---------------------------------------------------------------------
4||| 112500000.0 |   75.0 |    0.0 |    0.0 |    75.0 |    0.0 |pe.0
4||| 112500000.0 |   75.0 |    0.0 |    0.0 |    75.0 |    0.0 |pe.8
4||| 112500000.0 |   75.0 |    0.0 |    0.0 |    75.0 |    0.0 |pe.15
||||=====================================================================
|| 112500000.0 |   75.0 |    0.0 |    0.0 |    75.0 |    0.0 |#4.sweepz
3|             |        |        |        |         |        | sweepz_
4|             |        |        |        |         |        |  vhone_
|||||--------------------------------------------------------------------
5|||| 112500000.0 |   75.0 |    0.0 |    0.0 |    75.0 |    0.0 |pe.0
5|||| 112500000.0 |   75.0 |    0.0 |    0.0 |    75.0 |    0.0 |pe.8
5|||| 112500000.0 |   75.0 |    0.0 |    0.0 |    75.0 |    0.0 |pe.15
||=======================================================================
|  13825000.0 |   18.5 |    0.0 |    7.5 |     0.0 |   11.0 |mpi_gather
||-----------------------------------------------------------------------
||  13750000.0 |   11.0 |    0.0 |    0.0 |     0.0 |   11.0 |prin_
3|             |        |        |        |         |        | vhone_
||||---------------------------------------------------------------------
4|||  13750000.0 |   11.0 |    0.0 |    0.0 |     0.0 |   11.0 |pe.0
4|||  13750000.0 |   11.0 |    0.0 |    0.0 |     0.0 |   11.0 |pe.8
4|||  13750000.0 |   11.0 |    0.0 |    0.0 |     0.0 |   11.0 |pe.15
||||=====================================================================
||     37500.0 |    3.8 |    0.0 |    3.8 |     0.0 |    0.0 |imagesxy_
3|             |        |        |        |         |        | vhone_
||||---------------------------------------------------------------------
4|||    150000.0 |   15.0 |    0.0 |   15.0 |     0.0 |    0.0 |pe.0
4|||         0.0 |    0.0 |    0.0 |    0.0 |     0.0 |    0.0 |pe.8
4|||         0.0 |    0.0 |    0.0 |    0.0 |     0.0 |    0.0 |pe.15
||||=====================================================================
||     37500.0 |    3.8 |    0.0 |    3.8 |     0.0 |    0.0 |imagesxz_
3|             |        |        |        |         |        | vhone_
||||---------------------------------------------------------------------
4|||    150000.0 |   15.0 |    0.0 |   15.0 |     0.0 |    0.0 |pe.0
4|||         0.0 |    0.0 |    0.0 |    0.0 |     0.0 |    0.0 |pe.6
4|||         0.0 |    0.0 |    0.0 |    0.0 |     0.0 |    0.0 |pe.15
||=======================================================================
|       304.0 |   76.0 |   76.0 |    0.0 |     0.0 |    0.0 |MPI_ALLREDUCE
||-----------------------------------------------------------------------
||       300.0 |   75.0 |   75.0 |    0.0 |     0.0 |    0.0 |dtcon_
3|             |        |        |        |         |        | vhone_
||||---------------------------------------------------------------------
4|||       300.0 |   75.0 |   75.0 |    0.0 |     0.0 |    0.0 |pe.0
4|||       300.0 |   75.0 |   75.0 |    0.0 |     0.0 |    0.0 |pe.8
4|||       300.0 |   75.0 |   75.0 |    0.0 |     0.0 |    0.0 |pe.15
||||=====================================================================
||         4.0 |    1.0 |    1.0 |    0.0 |     0.0 |    0.0 |init_
3|             |        |        |        |         |        | vhone_
||||---------------------------------------------------------------------
4|||         4.0 |    1.0 |    1.0 |    0.0 |     0.0 |    0.0 |pe.0
4|||         4.0 |    1.0 |    1.0 |    0.0 |     0.0 |    0.0 |pe.8
4|||         4.0 |    1.0 |    1.0 |    0.0 |     0.0 |    0.0 |pe.15
|========================================================================


Notes for table 5:

  Table option:
    -O program_time
  Options implied by table option:
    -d pt,hm -b pe=[mmm]
  Other options:
    -T 

  The Total value for Process HiMem (MBytes), Process Time is the avg for the PE values.
    (To specify different aggregations, see: pat_help report options s1)



Table 5:  Wall Clock Time, Memory High Water Mark

  Process  |  Process  |PE=[mmm] 
     Time  |    HiMem  |
           | (MBytes)  |
          
 23.239839 |    34.380 |Total
|--------------------------------
| 23.240370 |    31.691 |pe.7
| 23.239757 |    31.848 |pe.9
| 23.239515 |    32.871 |pe.14
|================================

=========  Additional details ============================

Experiment:  trace

Original path to data file:
  /lustre/widow3/scratch/levesque/VH1_version1/vhone+pat+7694-11446t.xf  (RTS)

Original program:  /lustre/widow3/scratch/levesque/VH1_version1/vhone

Instrumented with:  pat_build -w -g mpi vhone 

Instrumented program:  ./vhone+pat

Program invocation:  ./vhone+pat 

Exit Status:  0 for 16 PEs

CPU  Family: 15h  Model: 01h  Stepping: 2

Core Performance Boost:  Configured for  0 PEs
                         Capable    for 16 PEs

Memory pagesize:  4096

Accelerator Model: Nvidia X2090 Memory: 6.00 GB Frequency: 1.15 GHz

Programming environment:  CRAY

Runtime environment variables:
  PBS_VERSION=TORQUE-2.5.9-snap.201111021154
  MODULE_VERSION=3.2.6.6
  MODULE_VERSION_STACK=3.2.6.6
  ASYNCPE_VERSION=5.05
  ATP_HOME=/opt/cray/atp/1.4.1
  ATP_MRNET_COMM_PATH=/opt/cray/atp/1.4.1/bin/atp_mrnet_commnode_wrapper
  ATP_POST_LINK_OPTS=-Wl,-L/opt/cray/atp/1.4.1/lib/ -Wl,-lAtpSigHandler -Wl,--undefined=__atpHandlerInstall
  LIBSCI_VERSION=11.0.04.4
  MPICH_ABORT_ON_ERROR=1
  PGI_VERSION=12.1
  XTOS_VERSION=4.0.30
  CRAY_MPICH2_VERSION=5.4.1
  MPICH_DIR=/opt/cray/mpt/5.4.1/xt/gemini/mpich2-pgi/109
  PAT_RT_HWPC=12

Report time environment variables:
  CRAYPAT_ROOT=/opt/cray/perftools/5.3.1
  PAT_REPORT_PRUNE_NAME=_cray$mt_start_,__cray_hwpc_,f_cray_hwpc_,cstart,__pat_,pat_region_,PAT_,OMP.slave_loop,slave_entry,_new_slave_entry,__libc_start_main,_start,__start,start_thread,__wrap_,UPC_ADIO_,_upc_,upc_,__caf_,__pgas_

Report command line options:  -T

Operating system:
  Linux 2.6.32.45-0.3.2_1.0400.6221-cray_gem_c #1 SMP Wed Sep 28 03:54:16 UTC 2011

Hardware performance counter events:
   DISPATCHED_FPU_OPS:OPS_DUAL_PIPE0:OPS_DUAL_PIPE1:OPS_DUAL_PIPE2:OPS_DUAL_PIPE3  FPU Pipe Assignment
   CYCLES_RTC                                                                      User Cycles (approx, from rtc)

Estimated minimum overhead per call of a traced function,
  which was subtracted from the data shown in this report
  (for raw data, use the option:  -s overhead=include):
    DISPATCHED_FPU_OPS:OPS_DUAL_PIPE0:OPS_DUAL_PIPE1:OPS_DUAL_PIPE2:OPS_DUAL_PIPE3  0.000  
    CYCLES_RTC                                                                   3156.959  
    Time                                                                            1.420  microsecs

Number of traced functions:  92

  (To see the list, specify:  -s traced_functions=show)

