%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                          S u m m a r y   R e p o r t
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Compilation
-----------
File     : /lustre/scratch/csep44/himeno/himeno_step5/himenoBMTxpr.f
Compiled : 07/16/13  15:14:33
Compiler : Version 8.1.8
Ftnlx    : Version 8128 (libcif 81032)
Target   : x86-64
Command  : /opt/cray/cce/8.1.8/cftn/x86-64/bin/ftn_driver.exe -hcpu=interlagos
           -haccel=nvidia_35 -hnetwork=gemini -hdynamic
           -I/sw/xk6/vampirtrace/5.14.3-chester/cle4.1_cray8.1.8/include/vampirt
           race -hfunc_trace -rm -eF -DGPU -ohimeno_acc
           -L/sw/xk6/vampirtrace/5.14.3-chester/cle4.1_cray8.1.8/lib -lfmpich
           -lvt-hyb -lvt-mpi-unify -lotfaux
           -L/opt/cray/cce/8.1.8/CC/x86-64/lib/x86-64 -lcray-c++-rts
           -lcraystdc++ -lmpich -lopen-trace-format -lz
           -L/opt/cray/papi/5.1.0.2/perf_events/no-cuda/lib/ -lpapi -ldl
           -L/opt/nvidia/cudatoolkit/5.0.35/extras/CUPTI/lib64/ -lcupti
           -L/opt/nvidia/cudatoolkit/5.0.35/lib64/ -lcuda -lcudart
           -I/opt/nvidia/cudatoolkit/5.0.35/include
           -I/opt/nvidia/cudatoolkit/5.0.35/extras/CUPTI/include
           -I/opt/nvidia/cudatoolkit/5.0.35/extras/Debugger/include
           -I/opt/cray/papi/5.1.0.2/perf_events/no-cuda/include
           -I/opt/cray/udreg/2.3.2-1.0401.5929.3.3.gem/include
           -I/opt/cray/ugni/4.0-1.0401.5928.9.5.gem/include
           -I/opt/cray/dmapp/3.2.1-1.0401.5983.4.5.gem/include
           -I/opt/cray/gni-headers/2.1-1.0401.5675.4.4.gem/include
           -I/opt/cray/xpmem/0.1-2.0401.36790.4.3.gem/include
           -I/opt/cray/pmi/4.0.1-1.0000.9421.73.3.gem/include
           -I/opt/cray/rca/1.0.0-2.0401.38656.2.2.gem/include
           -I/opt/cray-hss-devel/7.0.0/include
           -I/opt/cray/krca/1.0.0-2.0401.36792.3.70.gem/include
           -I/opt/cray/cce/8.1.8/craylibs/x86-64/include -haccel=nvidia_35
           -L/opt/nvidia/cudatoolkit/5.0.35/lib64
           -L/opt/nvidia/cudatoolkit/5.0.35/extras/CUPTI/lib64
           -L/opt/cray/nvidia/default/lib64 -lcuda
           -L/opt/cray/papi/5.1.0.2/perf_events/no-cuda/lib -lpapi
           -L/opt/cray/udreg/2.3.2-1.0401.5929.3.3.gem/lib64
           -L/opt/cray/ugni/4.0-1.0401.5928.9.5.gem/lib64
           -L/opt/cray/dmapp/3.2.1-1.0401.5983.4.5.gem/lib64
           -L/opt/cray/xpmem/0.1-2.0401.36790.4.3.gem/lib64
           -L/opt/cray/pmi/4.0.1-1.0000.9421.73.3.gem/lib64
           -L/opt/cray/rca/1.0.0-2.0401.38656.2.2.gem/lib64 -lrca
           -L/opt/cray/cce/8.1.8/craylibs/x86-64 -L/opt/gcc/4.4.4/snos/lib64
           -D__CRAYXE -D__CRAYXT_COMPUTE_LINUX_TARGET -D__TARGET_LINUX__
           -I/opt/cray/mpt/5.6.5/gni/mpich2-cray/74/include
           -I/opt/cray/libsci_acc/2.0.02/cray/81/x86_64/include
           -I/opt/cray/libsci/12.0.01/cray/74/interlagos/include
           -L/opt/cray/mpt/5.6.5/gni/mpich2-cray/74/lib
           -L/opt/cray/libsci_acc/2.0.02/cray/81/x86_64/lib
           -L/opt/cray/libsci/12.0.01/cray/74/interlagos/lib
           -lsci_acc_cray_nv35 -lscicpp_cray -lsci_cray_mp -lcufft -lcublas
           -lmpichf90_cray -lmpich_cray -lmpl -lpmi -lalpslli -lalpsutil
           -lpthread -lstdc++ -L/usr/lib/alps himenoBMTxpr.f

ftnlx report
------------
Source   : /lustre/scratch/csep44/himeno/himeno_step5/himenoBMTxpr.f
Date     : 07/16/2013  15:14:47


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                          S o u r c e   L i s t i n g
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


     %%%    L o o p m a r k   L e g e n d    %%%

     Primary Loop Type        Modifiers
     ------- ---- ----        ---------
     A - Pattern matched      a - atomic memory operation
                              b - blocked
     C - Collapsed            c - conditional and/or computed
     D - Deleted               
     E - Cloned               f - fused
     G - Accelerated          g - partitioned
     I - Inlined              i - interchanged
     M - Multithreaded        m - partitioned
                              n - non-blocking remote transfer
                              p - partial
                              r - unrolled
                              s - shortloop
     V - Vectorized           w - unwound

     + - More messages listed at end of listing
     ------------------------------------------


    1.                   C*********************************************************************
    2.                   C
    3.                   C This benchmark test program is measuring a cpu performance
    4.                   C of floating point operation by a Poisson equation solver.
    5.                   CC
    6.                   C If you have any question, please ask me via email.
    7.                   C written by Ryutaro HIMENO, November 26, 2001.
    8.                   C Version 3.0
    9.                   C ----------------------------------------------
   10.                   C Ryutaro Himeno, Dr. of Eng.
   11.                   C Head of Computer Information Division,
   12.                   C RIKEN (The Institute of Pysical and Chemical Research)
   13.                   C Email : himeno@postman.riken.go.jp
   14.                   C -----------------------------------------------------------
   15.                   C You can adjust the size of this benchmark code to fit your target
   16.                   C computer. In that case, please chose following sets of
   17.                   C (mimax,mjmax,mkmax):
   18.                   C small : 65,33,33
   19.                   C small : 129,65,65
   20.                   C midium: 257,129,129
   21.                   C large : 513,257,257
   22.                   C ext.large: 1025,513,513
   23.                   C This program is to measure a computer performance in MFLOPS
   24.                   C by using a kernel which appears in a linear solver of pressure
   25.                   C Poisson eq. which appears in an incompressible Navier-Stokes solver.
   26.                   C A point-Jacobi method is employed in this solver as this method can 
   27.                   C be easyly vectrized and be parallelized.
   28.                   C ------------------
   29.                   C Finite-difference method, curvilinear coodinate system
   30.                   C Vectorizable and parallelizable on each grid point
   31.                   C No. of grid points : imax x jmax x kmax including boundaries
   32.                   C ------------------
   33.                   C A,B,C:coefficient matrix, wrk1: source term of Poisson equation
   34.                   C wrk2 : working area, OMEGA : relaxation parameter
   35.                   C BND:control variable for boundaries and objects ( = 0 or 1)
   36.                   C P: pressure
   37.                   C -------------------
   38.                         PROGRAM HIMENOBMTXP
   39.                   C
   40.                         IMPLICIT REAL*4(a-h,o-z)
   41.                   C
   42.                         include 'mpif.h'
   43.                         include 'param.h'
   44.                   C
   45.                   C     ttarget specifys the measuring period in sec
   46.                         PARAMETER (ttarget=60.0)
   47.                   C
   48.                         real*8  cpu,cpu0,cpu1,xmflops2,flop
   49.                   C
   50.                         omega=0.8
   51.                         mx= mx0-1
   52.                         my= my0-1
   53.                         mz= mz0-1
   54.                   C
   55.                   CC Initializing communicator
   56.                   #ifdef GPU
   57.  + G------------< !$acc data create(a,p,b,c,bnd,wrk1,wrk2)
   58.    G              #endif
   59.  + G                    call initcomm
   60.    G              C
   61.    G              CC Initializaing computational index
   62.  + G                    call initmax(mx,my,mz,it)
   63.    G              C
   64.    G              CC Initializing matrixes
   65.  + G                    call initmt(mz,it)
   66.    G                    if(id .eq. 0) then
   67.    G                       write(*,*) 'Sequential version array size'
   68.    G                       write(*,*) ' mimax=',mx0,' mjmax=',my0,' mkmax=',mz0
   69.    G                       write(*,*) 'Parallel version  array size'
   70.    G                       write(*,*) ' mimax=',mimax,' mjmax=',mjmax,' mkmax=',mkmax
   71.    G                       write(*,*) ' imax=',imax,' jmax=',jmax,' kmax=',kmax
   72.    G                       write(*,*) ' I-decomp= ',ndx,' J-decomp= ',ndy,
   73.    G                   >              ' K-decomp= ',ndz
   74.    G                       write(*,*)
   75.    G                    end if
   76.    G              C
   77.    G              CC Start measuring
   78.    G              C
   79.    G                    nn=3
   80.    G                    if(id .eq. 0) then
   81.    G                       write(*,*) ' Start rehearsal measurement process.'
   82.    G                       write(*,*) ' Measure the performance in 3 times.'
   83.    G                    end if
   84.    G              C
   85.    G                    gosa= 0.0
   86.    G                    cpu= 0.0
   87.  + G                    call mpi_barrier(mpi_comm_world,ierr)
   88.  + G                    cpu0= mpi_wtime()
   89.    G              C Jacobi iteration
   90.  + G                    call jacobi(nn,gosa)
   91.  + G                    cpu1= mpi_wtime() - cpu0
   92.    G              C
   93.  + G                    call mpi_allreduce(cpu1,
   94.    G                   >                   cpu,
   95.    G                   >                   1,
   96.    G                   >                   mpi_real8,
   97.    G                   >                   mpi_max,
   98.    G                   >                   mpi_comm_world,
   99.    G                   >                   ierr)
  100.    G              C
  101.    G                    flop=real(mx-2)*real(my-2)*real(mz-2)*34.0
  102.    G                    if(cpu .ne. 0.0) xmflops2=flop/cpu*1.0d-6*real(nn)
  103.    G                    if(id .eq. 0) then
  104.    G                       write(*,*) '  MFLOPS:',xmflops2,'  time(s):',cpu,gosa
  105.    G                    end if
  106.  + G                    nn= int(ttarget/(cpu/3.0))
  107.    G              C
  108.    G              C     end the test loop
  109.    G                    if(id .eq. 0) then
  110.    G                       write(*,*) 'Now, start the actual measurement process.'
  111.    G                       write(*,*) 'The loop will be excuted in',nn,' times.'
  112.    G                       write(*,*) 'This will take about one minute.'
  113.    G                       write(*,*) 'Wait for a while.'
  114.    G                    end if
  115.    G              C
  116.    G                    gosa= 0.0
  117.    G                    cpu= 0.0
  118.  + G                    call mpi_barrier(mpi_comm_world,ierr)
  119.  + G                    cpu0= mpi_wtime()
  120.    G              C Jacobi iteration
  121.  + G                    call jacobi(nn,gosa)
  122.  + G                    cpu1= mpi_wtime() - cpu0
  123.    G              C
  124.  + G                    call mpi_reduce(cpu1,
  125.    G                   >                cpu,
  126.    G                   >                1,
  127.    G                   >                mpi_real8,
  128.    G                   >                mpi_max,
  129.    G                   >                0,
  130.    G                   >                mpi_comm_world,
  131.    G                   >                ierr)
  132.    G              C
  133.    G                    if(id .eq. 0) then
  134.    G                       if(cpu .ne. 0.0)  xmflops2=flop*1.0d-6/cpu*real(nn)
  135.    G              C
  136.    G                       write(*,*) ' Loop executed for ',nn,' times'
  137.    G                       write(*,*) ' Gosa :',gosa
  138.    G                       write(*,*) ' MFLOPS:',xmflops2, '  time(s):',cpu
  139.  + G                       score=xmflops2/82.84
  140.    G                       write(*,*) ' Score based on Pentium III 600MHz :',score
  141.    G                    end if
  142.  + G                    call mpi_finalize(ierr)
  143.    G              #ifdef GPU
  144.    G------------> !$acc end data 
  145.                   #endif
  146.                   C
  147.                         stop
  148.                         END

ftn-6413 ftn: ACCEL File = himenoBMTxpr.f, Line = 57 
  A data region was created at line 57 and ending at line 144.

ftn-6421 ftn: ACCEL File = himenoBMTxpr.f, Line = 57 
  Allocate memory for whole array "wrk2" on accelerator, free at line 144 (acc_share).

ftn-6421 ftn: ACCEL File = himenoBMTxpr.f, Line = 57 
  Allocate memory for whole array "wrk1" on accelerator, free at line 144 (acc_share).

ftn-6421 ftn: ACCEL File = himenoBMTxpr.f, Line = 57 
  Allocate memory for whole array "bnd" on accelerator, free at line 144 (acc_share).

ftn-6421 ftn: ACCEL File = himenoBMTxpr.f, Line = 57 
  Allocate memory for whole array "c" on accelerator, free at line 144 (acc_share).

ftn-6421 ftn: ACCEL File = himenoBMTxpr.f, Line = 57 
  Allocate memory for whole array "b" on accelerator, free at line 144 (acc_share).

ftn-6421 ftn: ACCEL File = himenoBMTxpr.f, Line = 57 
  Allocate memory for whole array "p" on accelerator, free at line 144 (acc_share).

ftn-6421 ftn: ACCEL File = himenoBMTxpr.f, Line = 57 
  Allocate memory for whole array "a" on accelerator, free at line 144 (acc_share).

ftn-3118 ftn: IPA File = himenoBMTxpr.f, Line = 59 
  "initcomm" (called from "HIMENOBMTXP") was not inlined because the call site will not flatten.  Routine "mpi_init" is missing.

ftn-3118 ftn: IPA File = himenoBMTxpr.f, Line = 62 
  "initmax" (called from "HIMENOBMTXP") was not inlined because the call site will not flatten.  Routine "mpi_type_vector" is
  missing.

ftn-3171 ftn: IPA File = himenoBMTxpr.f, Line = 65 
  "initmt" (called from "HIMENOBMTXP") was not inlined because it is not in the body of a loop.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 87 
  "mpi_barrier" (called from "HIMENOBMTXP") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 88 
  "mpi_wtime" (called from "HIMENOBMTXP") was not inlined because the compiler was unable to locate the routine.

ftn-3118 ftn: IPA File = himenoBMTxpr.f, Line = 90 
  "jacobi" (called from "HIMENOBMTXP") was not inlined because the call site will not flatten.  Routine "mpi_irecv" is missing.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 91 
  "mpi_wtime" (called from "HIMENOBMTXP") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 93 
  "mpi_allreduce" (called from "HIMENOBMTXP") was not inlined because the compiler was unable to locate the routine.

ftn-6010 ftn: SCALAR File = himenoBMTxpr.f, Line = 106 
  A divide was turned into a multiply by a reciprocal

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 118 
  "mpi_barrier" (called from "HIMENOBMTXP") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 119 
  "mpi_wtime" (called from "HIMENOBMTXP") was not inlined because the compiler was unable to locate the routine.

ftn-3118 ftn: IPA File = himenoBMTxpr.f, Line = 121 
  "jacobi" (called from "HIMENOBMTXP") was not inlined because the call site will not flatten.  Routine "mpi_irecv" is missing.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 122 
  "mpi_wtime" (called from "HIMENOBMTXP") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 124 
  "mpi_reduce" (called from "HIMENOBMTXP") was not inlined because the compiler was unable to locate the routine.

ftn-6010 ftn: SCALAR File = himenoBMTxpr.f, Line = 139 
  A divide was turned into a multiply by a reciprocal

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 142 
  "mpi_finalize" (called from "HIMENOBMTXP") was not inlined because the compiler was unable to locate the routine.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                          S o u r c e   L i s t i n g
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  149.                   C
  150.                   C
  151.                   C**************************************************************
  152.                         subroutine initmt(mz,it)
  153.                   C**************************************************************
  154.                         IMPLICIT REAL*4(a-h,o-z)
  155.                   C
  156.                         include 'param.h'
  157.                   C
  158.                   #ifdef GPU
  159.    G------------< !$acc data present(a,p,b,c,bnd,wrk1,wrk2)
  160.    G G----------< !$ACC  parallel loop 
  161.    G G            !$ACC&   private (i,j,k)                             
  162.    G G            #else
  163.    G G            ! Directive inserted by Cray Reveal.  May be incomplete.
  164.    G G            !$OMP  parallel do default(none)                      
  165.    G G            !$OMP&   private (i,j,k)                             
  166.    G G            !$OMP&   shared  (a,b,bnd,c,p,wrk1,wrk2)
  167.    G G            #endif
  168.    G G g--------<       do k=1,mkmax
  169.  + G G g C------<          do j=1,mjmax
  170.    G G g C gC---<             do i=1,mimax
  171.    G G g C gC                    a(i,j,k,1)=0.0
  172.    G G g C gC                    a(i,j,k,2)=0.0
  173.    G G g C gC                    a(i,j,k,3)=0.0
  174.    G G g C gC                    a(i,j,k,4)=0.0
  175.    G G g C gC                    b(i,j,k,1)=0.0
  176.    G G g C gC                    b(i,j,k,2)=0.0
  177.    G G g C gC                    b(i,j,k,3)=0.0
  178.    G G g C gC                    c(i,j,k,1)=0.0
  179.    G G g C gC                    c(i,j,k,2)=0.0
  180.    G G g C gC                    c(i,j,k,3)=0.0
  181.    G G g C gC                    p(i,j,k)=0.0
  182.    G G g C gC                    wrk1(i,j,k)=0.0   
  183.    G G g C gC                    wrk2(i,j,k)=0.0   
  184.    G G g C gC                    bnd(i,j,k)=0.0 
  185.    G G g C gC--->             enddo
  186.    G G g C------>          enddo
  187.    G G g-------->       enddo
  188.    G G            #ifdef GPU
  189.    G G----------> !$ACC  end parallel loop 
  190.    G              #endif
  191.    G              C
  192.    G              #ifdef GPU
  193.    G G----------< !$ACC  parallel loop 
  194.    G G            !$ACC&   private (i,j,k)                             
  195.    G G            #else
  196.    G G            ! Directive inserted by Cray Reveal.  May be incomplete.
  197.    G G            !$OMP  parallel do 
  198.    G G            !$OMP&   private (i,j,k)                             
  199.    G G            !$OMP&   shared  (a,b,bnd,c,p,wrk1,wrk2)
  200.    G G            #endif
  201.    G G g--------<       do k=1,kmax
  202.  + G G g 4------<          do j=1,jmax
  203.    G G g 4 g----<             do i=1,imax
  204.    G G g 4 g                     a(i,j,k,1)=1.0
  205.    G G g 4 g                     a(i,j,k,2)=1.0
  206.    G G g 4 g                     a(i,j,k,3)=1.0
  207.    G G g 4 g                     a(i,j,k,4)=1.0/6.0
  208.    G G g 4 g                     b(i,j,k,1)=0.0
  209.    G G g 4 g                     b(i,j,k,2)=0.0
  210.    G G g 4 g                     b(i,j,k,3)=0.0
  211.    G G g 4 g                     c(i,j,k,1)=1.0
  212.    G G g 4 g                     c(i,j,k,2)=1.0
  213.    G G g 4 g                     c(i,j,k,3)=1.0
  214.    G G g 4 g                     p(i,j,k)=float((k-1+it)*(k-1+it))
  215.    G G g 4 g           >                       /float((mz-1)*(mz-1))
  216.    G G g 4 g                     wrk1(i,j,k)=0.0   
  217.    G G g 4 g                     wrk2(i,j,k)=0.0   
  218.    G G g 4 g                     bnd(i,j,k)=1.0
  219.    G G g 4 g---->             enddo
  220.    G G g 4------>          enddo
  221.    G G g-------->       enddo
  222.    G G            #ifdef GPU
  223.    G G----------> !$ACC  end parallel loop 
  224.    G              #endif
  225.    G              #ifdef GPU
  226.    G------------> !$acc end data 
  227.                   #endif
  228.                   C
  229.                         return
  230.                         end

ftn-6413 ftn: ACCEL File = himenoBMTxpr.f, Line = 159 
  A data region was created at line 159 and ending at line 226.

ftn-6405 ftn: ACCEL File = himenoBMTxpr.f, Line = 160 
  A region starting at line 160 and ending at line 189 was placed on the accelerator.

ftn-6430 ftn: ACCEL File = himenoBMTxpr.f, Line = 168 
  A loop starting at line 168 was partitioned across the thread blocks.

ftn-6509 ftn: ACCEL File = himenoBMTxpr.f, Line = 169 
  A loop starting at line 169 was not partitioned because a better candidate was found at line 170.

ftn-6003 ftn: SCALAR File = himenoBMTxpr.f, Line = 169 
  A loop starting at line 169 was collapsed into the loop starting at line 170.

ftn-6430 ftn: ACCEL File = himenoBMTxpr.f, Line = 170 
  A loop starting at line 170 was partitioned across the 128 threads within a threadblock.

ftn-6405 ftn: ACCEL File = himenoBMTxpr.f, Line = 193 
  A region starting at line 193 and ending at line 223 was placed on the accelerator.

ftn-6430 ftn: ACCEL File = himenoBMTxpr.f, Line = 201 
  A loop starting at line 201 was partitioned across the thread blocks.

ftn-6509 ftn: ACCEL File = himenoBMTxpr.f, Line = 202 
  A loop starting at line 202 was not partitioned because a better candidate was found at line 203.

ftn-6412 ftn: ACCEL File = himenoBMTxpr.f, Line = 202 
  A loop starting at line 202 will be redundantly executed.

ftn-6430 ftn: ACCEL File = himenoBMTxpr.f, Line = 203 
  A loop starting at line 203 was partitioned across the 128 threads within a threadblock.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                          S o u r c e   L i s t i n g
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  231.                   C
  232.                   C*************************************************************
  233.                         subroutine jacobi(nn,gosa)
  234.                   C*************************************************************
  235.                         IMPLICIT REAL*4(a-h,o-z)
  236.                   C
  237.                         include 'mpif.h'
  238.                         include 'param.h'
  239.                         real pack_p11(kmax*jmax),pack_p12(kmax*jmax)
  240.                         real pack_p21(kmax*imax),pack_p22(kmax*imax)
  241.                         real pack_p31(imax*jmax),pack_p32(imax*jmax)
  242.                         real unpack_p11(kmax*jmax),unpack_p12(kmax*jmax)
  243.                         real unpack_p21(kmax*imax),unpack_p22(kmax*imax)
  244.                         real unpack_p31(imax*jmax),unpack_p32(imax*jmax)
  245.                        
  246.                   #ifdef GPU
  247.  + G------------< !$acc data present(a,p,b,c,bnd,wrk1,wrk2) 
  248.    G              !$acc&        present_or_create(wgosa,
  249.    G              !$acc&             pack_p11,pack_p12,pack_p21,pack_p22, 
  250.    G              !$acc&             pack_p31,pack_p32,unpack_p11,unpack_p12, 
  251.    G              !$acc&         unpack_p21,unpack_p22,unpack_p31,unpack_p32) 
  252.    G              #endif
  253.    G              C
  254.  + G 2----------<       DO loop=1,nn
  255.    G 2                     gosa=0.0
  256.    G 2                     wgosa=0.0
  257.    G 2            ! Directive inserted by Cray Reveal.  May be incomplete.
  258.    G 2            #ifdef GPU
  259.    G 2 G--------< !$ACC  parallel loop 
  260.    G 2 G          !$ACC&   private (i,j,k,s0,ss)                                    
  261.    G 2 G          !$ACC&   reduction (+:wgosa)
  262.    G 2 G          #else
  263.    G 2 G          !$OMP  parallel do default(none)                                   
  264.    G 2 G          !$OMP&   private (i,j,k,s0,ss)                                    
  265.    G 2 G          !$OMP&   shared  (a,b,bnd,c,imax,jmax,kmax,omega,p,wrk1,wrk2)    
  266.    G 2 G          !$OMP&   reduction (+:wgosa)
  267.    G 2 G          #endif
  268.    G 2 G g------<          DO K=2,kmax-1
  269.  + G 2 G g 5----<             DO J=2,jmax-1
  270.    G 2 G g 5 g--<                DO I=2,imax-1
  271.    G 2 G g 5 g                      S0=a(I,J,K,1)*p(I+1,J,K)+a(I,J,K,2)*p(I,J+1,K)
  272.    G 2 G g 5 g         1                 +a(I,J,K,3)*p(I,J,K+1)
  273.    G 2 G g 5 g         2                 +b(I,J,K,1)*(p(I+1,J+1,K)-p(I+1,J-1,K)
  274.    G 2 G g 5 g         3                 -p(I-1,J+1,K)+p(I-1,J-1,K))
  275.    G 2 G g 5 g         4                 +b(I,J,K,2)*(p(I,J+1,K+1)-p(I,J-1,K+1)
  276.    G 2 G g 5 g         5                 -p(I,J+1,K-1)+p(I,J-1,K-1))
  277.    G 2 G g 5 g         6                 +b(I,J,K,3)*(p(I+1,J,K+1)-p(I-1,J,K+1)
  278.    G 2 G g 5 g         7                 -p(I+1,J,K-1)+p(I-1,J,K-1))
  279.    G 2 G g 5 g         8                 +c(I,J,K,1)*p(I-1,J,K)+c(I,J,K,2)*p(I,J-1,K)
  280.    G 2 G g 5 g         9                 +c(I,J,K,3)*p(I,J,K-1)+wrk1(I,J,K)
  281.    G 2 G g 5 g                      SS=(S0*a(I,J,K,4)-p(I,J,K))*bnd(I,J,K)
  282.    G 2 G g 5 g                      WGOSA=WGOSA+SS*SS
  283.    G 2 G g 5 g                      wrk2(I,J,K)=p(I,J,K)+OMEGA *SS
  284.    G 2 G g 5 g-->                enddo
  285.    G 2 G g 5---->             enddo
  286.    G 2 G g------>          enddo
  287.    G 2 G          #ifdef GPU
  288.    G 2 G--------> !$ACC  end parallel loop 
  289.    G 2            #endif
  290.    G 2            C     
  291.    G 2            #ifdef GPU
  292.    G 2 G--------< !$ACC  parallel loop 
  293.    G 2 G          !$ACC&   private (i,j,k)                                    
  294.    G 2 G          #else
  295.    G 2 G          !$OMP  parallel do 
  296.    G 2 G          !$OMP&   private (i,j,k)                                    
  297.    G 2 G          !$OMP&   shared  (p,wrk2)    
  298.    G 2 G          #endif
  299.    G 2 G g------<          DO K=2,kmax-1
  300.  + G 2 G g 5----<             DO J=2,jmax-1
  301.    G 2 G g 5 g--<                DO I=2,imax-1
  302.    G 2 G g 5 g                      p(I,J,K)=wrk2(I,J,K)
  303.    G 2 G g 5 g-->                enddo
  304.    G 2 G g 5---->             enddo
  305.    G 2 G g------>          enddo
  306.    G 2 G          #ifdef GPU
  307.    G 2 G--------> !$ACC  end parallel loop 
  308.    G 2            #endif
  309.    G 2            #ifdef GPU
  310.    G 2 G--------< !$ACC  parallel loop 
  311.    G 2 G          !$ACC&   private (ii,j,k)                                    
  312.    G 2 G          #else
  313.    G 2 G          !$OMP  parallel do 
  314.    G 2 G          !$OMP&   private (ii,j,k)                                    
  315.    G 2 G          !$OMP&   shared  (p,wrk2)    
  316.    G 2 G          #endif
  317.    G 2 G g------<          DO K=1,kmax
  318.  + G 2 G g r4---<             DO J=1,jmax
  319.    G 2 G g r4                     ii = j + (k-1)*jmax
  320.    G 2 G g r4                       pack_p11(ii)=p(2,j,k)
  321.    G 2 G g r4                       pack_p12(ii)=p(imax-1,j,k)
  322.    G 2 G g r4--->             enddo
  323.    G 2 G g------>          enddo
  324.    G 2 G          #ifdef GPU
  325.    G 2 G--------> !$ACC  end parallel loop 
  326.    G 2            #endif
  327.    G 2            #ifdef GPU
  328.    G 2 G--------< !$ACC  parallel loop 
  329.    G 2 G          !$ACC&   private (i,jj,k)                                    
  330.    G 2 G          #else
  331.    G 2 G          !$OMP  parallel do 
  332.    G 2 G          !$OMP&   private (i,jj,k)                                    
  333.    G 2 G          #endif
  334.    G 2 G g------<          DO K=1,kmax
  335.  + G 2 G g r4---<             DO I=1,imax
  336.    G 2 G g r4                     jj = i +(k-1)*imax
  337.    G 2 G g r4                       pack_p21(jj)=p(i,2,k)
  338.    G 2 G g r4                       pack_p22(jj)=p(i,jmax-1,k)
  339.    G 2 G g r4--->             enddo
  340.    G 2 G g------>          enddo
  341.    G 2 G          #ifdef GPU
  342.    G 2 G--------> !$ACC  end parallel loop 
  343.    G 2            #endif
  344.    G 2            #ifdef GPU
  345.    G 2 G--------< !$ACC  parallel loop 
  346.    G 2 G          !$ACC&   private (i,j,kk)                                    
  347.    G 2 G          #else
  348.    G 2 G          !$OMP  parallel do 
  349.    G 2 G          !$OMP&   private (i,j,kk)                                    
  350.    G 2 G          #endif
  351.    G 2 G g------<          DO J=1,jmax
  352.  + G 2 G g r4---<             DO I=1,imax
  353.    G 2 G g r4                     kk = i + (j-1)*imax
  354.    G 2 G g r4                       pack_p31(kk)=p(i,j,2)
  355.    G 2 G g r4                       pack_p32(kk)=p(i,j,kmax-1)
  356.    G 2 G g r4--->             enddo
  357.    G 2 G g------>          enddo
  358.    G 2 G          #ifdef GPU
  359.    G 2 G--------> !$ACC  end parallel loop 
  360.    G 2            #endif
  361.    G 2            #ifdef GPU
  362.    G 2            !$acc update host(pack_p11,pack_p12,pack_p21,pack_p22,
  363.    G 2            !$acc&              pack_p31,pack_p32,wgosa)
  364.    G 2            #endif
  365.    G 2            C
  366.    G 2                  if(id.eq.23)then
  367.    G 2                    print *,'in buffers',sum(pack_p11),sum(pack_p21),
  368.    G 2                 &                 sum(pack_p31)
  369.    G 2                    print *,'in buffers',sum(pack_p12),sum(pack_p22),
  370.    G 2                 &                 sum(pack_p32)
  371.    G 2                  endif
  372.  + G 2                     call sendp(ndx,ndy,ndz,
  373.    G 2                 & pack_p11,pack_p12,pack_p21,pack_p22,
  374.    G 2                 & pack_p31,pack_p32,unpack_p11,unpack_p12,
  375.    G 2                 & unpack_p21,unpack_p22,unpack_p31,unpack_p32)
  376.    G 2                  if(id.eq.23)then
  377.    G 2                    print *,'out buffers',sum(unpack_p11),sum(unpack_p21),
  378.    G 2                 &                 sum(unpack_p31)
  379.    G 2                    print *,'out buffers',sum(unpack_p12),sum(unpack_p22),
  380.    G 2                 &                 sum(unpack_p32)
  381.    G 2                  endif
  382.    G 2            c
  383.    G 2            #ifdef GPU
  384.    G 2            !$acc update device(unpack_p11,unpack_p12,unpack_p21,unpack_p22,
  385.    G 2            !$acc&              unpack_p31,unpack_p32)
  386.    G 2            #endif
  387.    G 2            C
  388.    G 2            #ifdef GPU
  389.    G 2 G--------< !$ACC  parallel loop 
  390.    G 2 G          !$ACC&   private (ii,j,k)                                    
  391.    G 2 G          #else
  392.    G 2 G          !$OMP  parallel do 
  393.    G 2 G          !$OMP&   private (ii,j,k)                                    
  394.    G 2 G          !$OMP&   shared  (p,wrk2)    
  395.    G 2 G          #endif
  396.    G 2 G g------<          DO K=1,kmax
  397.    G 2 G g g----<             DO J=1,jmax
  398.    G 2 G g g                      ii = j + (k-1)*jmax
  399.    G 2 G g g                        p(1,j,k)=unpack_p11(ii)
  400.    G 2 G g g                        p(imax,j,k)=unpack_p12(ii)
  401.    G 2 G g g---->             enddo
  402.    G 2 G g------>          enddo
  403.    G 2 G          #ifdef GPU
  404.    G 2 G--------> !$ACC  end parallel loop 
  405.    G 2            #endif
  406.    G 2            #ifdef GPU
  407.    G 2 G--------< !$ACC  parallel loop 
  408.    G 2 G          !$ACC&   private (i,jj,k)                                    
  409.    G 2 G          #else
  410.    G 2 G          !$OMP  parallel do 
  411.    G 2 G          !$OMP&   private (i,jj,k)                                    
  412.    G 2 G          #endif
  413.    G 2 G g------<          DO K=1,kmax
  414.    G 2 G g g----<             DO I=1,imax
  415.    G 2 G g g                      jj = i + (k-1)*imax
  416.    G 2 G g g                        p(i,1,k)=unpack_p21(jj)
  417.    G 2 G g g                        p(i,jmax,k)=unpack_p22(jj)
  418.    G 2 G g g---->             enddo
  419.    G 2 G g------>          enddo
  420.    G 2 G          #ifdef GPU
  421.    G 2 G--------> !$ACC  end parallel loop 
  422.    G 2            #endif
  423.    G 2            #ifdef GPU
  424.    G 2 G--------< !$ACC  parallel loop 
  425.    G 2 G          !$ACC&   private (i,j,kk)                                    
  426.    G 2 G          #else
  427.    G 2 G          !$OMP  parallel do 
  428.    G 2 G          !$OMP&   private (i,j,kk)                                    
  429.    G 2 G          #endif
  430.    G 2 G g------<          DO J=1,jmax
  431.    G 2 G g g----<             DO I=1,imax
  432.    G 2 G g g                      kk = i +(j-1)*imax 
  433.    G 2 G g g                        p(i,j,1)=unpack_p31(kk)
  434.    G 2 G g g                        p(i,j,kmax)=unpack_p32(kk)
  435.    G 2 G g g---->             enddo
  436.    G 2 G g------>          enddo
  437.    G 2 G          #ifdef GPU
  438.    G 2 G--------> !$ACC  end parallel loop 
  439.    G 2            #endif
  440.  + G 2                     call mpi_allreduce(wgosa,
  441.    G 2                 >                      gosa,
  442.    G 2                 >                      1,
  443.    G 2                 >                      mpi_real4,
  444.    G 2                 >                      mpi_sum,
  445.    G 2                 >                      mpi_comm_world,
  446.    G 2                 >                      ierr)
  447.    G 2            C
  448.    G 2---------->       enddo
  449.    G              #ifdef GPU
  450.    G------------> !$acc end data
  451.                   #endif
  452.                   CC End of iteration
  453.                         return
  454.                         end

ftn-6413 ftn: ACCEL File = himenoBMTxpr.f, Line = 247 
  A data region was created at line 247 and ending at line 450.

ftn-6422 ftn: ACCEL File = himenoBMTxpr.f, Line = 247 
  If not already present: allocate memory for whole array "unpack_p32" on accelerator, free at line 450 (acc_share).

ftn-6422 ftn: ACCEL File = himenoBMTxpr.f, Line = 247 
  If not already present: allocate memory for whole array "unpack_p31" on accelerator, free at line 450 (acc_share).

ftn-6422 ftn: ACCEL File = himenoBMTxpr.f, Line = 247 
  If not already present: allocate memory for whole array "unpack_p22" on accelerator, free at line 450 (acc_share).

ftn-6422 ftn: ACCEL File = himenoBMTxpr.f, Line = 247 
  If not already present: allocate memory for whole array "unpack_p21" on accelerator, free at line 450 (acc_share).

ftn-6422 ftn: ACCEL File = himenoBMTxpr.f, Line = 247 
  If not already present: allocate memory for whole array "unpack_p12" on accelerator, free at line 450 (acc_share).

ftn-6422 ftn: ACCEL File = himenoBMTxpr.f, Line = 247 
  If not already present: allocate memory for whole array "unpack_p11" on accelerator, free at line 450 (acc_share).

ftn-6422 ftn: ACCEL File = himenoBMTxpr.f, Line = 247 
  If not already present: allocate memory for whole array "pack_p32" on accelerator, free at line 450 (acc_share).

ftn-6422 ftn: ACCEL File = himenoBMTxpr.f, Line = 247 
  If not already present: allocate memory for whole array "pack_p31" on accelerator, free at line 450 (acc_share).

ftn-6422 ftn: ACCEL File = himenoBMTxpr.f, Line = 247 
  If not already present: allocate memory for whole array "pack_p22" on accelerator, free at line 450 (acc_share).

ftn-6422 ftn: ACCEL File = himenoBMTxpr.f, Line = 247 
  If not already present: allocate memory for whole array "pack_p21" on accelerator, free at line 450 (acc_share).

ftn-6422 ftn: ACCEL File = himenoBMTxpr.f, Line = 247 
  If not already present: allocate memory for whole array "pack_p12" on accelerator, free at line 450 (acc_share).

ftn-6422 ftn: ACCEL File = himenoBMTxpr.f, Line = 247 
  If not already present: allocate memory for whole array "pack_p11" on accelerator, free at line 450 (acc_share).

ftn-6422 ftn: ACCEL File = himenoBMTxpr.f, Line = 247 
  If not already present: allocate memory for variable "wgosa" on accelerator, free at line 450 (acc_share).

ftn-6286 ftn: VECTOR File = himenoBMTxpr.f, Line = 254 
  A loop starting at line 254 was not vectorized because it contains input/output operations at line 367.

ftn-6405 ftn: ACCEL File = himenoBMTxpr.f, Line = 259 
  A region starting at line 259 and ending at line 288 was placed on the accelerator.

ftn-6430 ftn: ACCEL File = himenoBMTxpr.f, Line = 268 
  A loop starting at line 268 was partitioned across the thread blocks.

ftn-6509 ftn: ACCEL File = himenoBMTxpr.f, Line = 269 
  A loop starting at line 269 was not partitioned because a better candidate was found at line 270.

ftn-6412 ftn: ACCEL File = himenoBMTxpr.f, Line = 269 
  A loop starting at line 269 will be redundantly executed.

ftn-6430 ftn: ACCEL File = himenoBMTxpr.f, Line = 270 
  A loop starting at line 270 was partitioned across the 128 threads within a threadblock.

ftn-6405 ftn: ACCEL File = himenoBMTxpr.f, Line = 292 
  A region starting at line 292 and ending at line 307 was placed on the accelerator.

ftn-6430 ftn: ACCEL File = himenoBMTxpr.f, Line = 299 
  A loop starting at line 299 was partitioned across the thread blocks.

ftn-6509 ftn: ACCEL File = himenoBMTxpr.f, Line = 300 
  A loop starting at line 300 was not partitioned because a better candidate was found at line 301.

ftn-6412 ftn: ACCEL File = himenoBMTxpr.f, Line = 300 
  A loop starting at line 300 will be redundantly executed.

ftn-6430 ftn: ACCEL File = himenoBMTxpr.f, Line = 301 
  A loop starting at line 301 was partitioned across the 128 threads within a threadblock.

ftn-6405 ftn: ACCEL File = himenoBMTxpr.f, Line = 310 
  A region starting at line 310 and ending at line 325 was placed on the accelerator.

ftn-6430 ftn: ACCEL File = himenoBMTxpr.f, Line = 317 
  A loop starting at line 317 was partitioned across the threadblocks and the 128 threads within a threadblock.

ftn-6005 ftn: SCALAR File = himenoBMTxpr.f, Line = 318 
  A loop starting at line 318 was unrolled 4 times.

ftn-6411 ftn: ACCEL File = himenoBMTxpr.f, Line = 318 
  A loop starting at line 318 will be serially executed.

ftn-6405 ftn: ACCEL File = himenoBMTxpr.f, Line = 328 
  A region starting at line 328 and ending at line 342 was placed on the accelerator.

ftn-6430 ftn: ACCEL File = himenoBMTxpr.f, Line = 334 
  A loop starting at line 334 was partitioned across the threadblocks and the 128 threads within a threadblock.

ftn-6005 ftn: SCALAR File = himenoBMTxpr.f, Line = 335 
  A loop starting at line 335 was unrolled 4 times.

ftn-6411 ftn: ACCEL File = himenoBMTxpr.f, Line = 335 
  A loop starting at line 335 will be serially executed.

ftn-6405 ftn: ACCEL File = himenoBMTxpr.f, Line = 345 
  A region starting at line 345 and ending at line 359 was placed on the accelerator.

ftn-6430 ftn: ACCEL File = himenoBMTxpr.f, Line = 351 
  A loop starting at line 351 was partitioned across the threadblocks and the 128 threads within a threadblock.

ftn-6005 ftn: SCALAR File = himenoBMTxpr.f, Line = 352 
  A loop starting at line 352 was unrolled 4 times.

ftn-6411 ftn: ACCEL File = himenoBMTxpr.f, Line = 352 
  A loop starting at line 352 will be serially executed.

ftn-3118 ftn: IPA File = himenoBMTxpr.f, Line = 372 
  "sendp" (called from "jacobi") was not inlined because the call site will not flatten.  Routine "mpi_irecv" is missing.

ftn-6405 ftn: ACCEL File = himenoBMTxpr.f, Line = 389 
  A region starting at line 389 and ending at line 404 was placed on the accelerator.

ftn-6430 ftn: ACCEL File = himenoBMTxpr.f, Line = 396 
  A loop starting at line 396 was partitioned across the thread blocks.

ftn-6430 ftn: ACCEL File = himenoBMTxpr.f, Line = 397 
  A loop starting at line 397 was partitioned across the 128 threads within a threadblock.

ftn-6405 ftn: ACCEL File = himenoBMTxpr.f, Line = 407 
  A region starting at line 407 and ending at line 421 was placed on the accelerator.

ftn-6430 ftn: ACCEL File = himenoBMTxpr.f, Line = 413 
  A loop starting at line 413 was partitioned across the thread blocks.

ftn-6430 ftn: ACCEL File = himenoBMTxpr.f, Line = 414 
  A loop starting at line 414 was partitioned across the 128 threads within a threadblock.

ftn-6405 ftn: ACCEL File = himenoBMTxpr.f, Line = 424 
  A region starting at line 424 and ending at line 438 was placed on the accelerator.

ftn-6430 ftn: ACCEL File = himenoBMTxpr.f, Line = 430 
  A loop starting at line 430 was partitioned across the thread blocks.

ftn-6430 ftn: ACCEL File = himenoBMTxpr.f, Line = 431 
  A loop starting at line 431 was partitioned across the 128 threads within a threadblock.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 440 
  "mpi_allreduce" (called from "jacobi") was not inlined because the compiler was unable to locate the routine.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                          S o u r c e   L i s t i n g
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  455.                   c
  456.                   c
  457.                   c
  458.                         subroutine initcomm
  459.                   c
  460.                         IMPLICIT REAL*4(a-h,o-z)
  461.                   c
  462.                         include 'mpif.h'
  463.                         include 'param.h'
  464.                   c
  465.                         logical    ipd(3),ir
  466.                         dimension  idm(3)
  467.                   c
  468.  +                      call mpi_init(ierr)
  469.  +                      call mpi_comm_size(mpi_comm_world,npe,ierr)
  470.  +                      call mpi_comm_rank(mpi_comm_world,id,ierr)
  471.                   C
  472.                         if(ndx*ndy*ndz .ne. npe) then
  473.                            if(id .eq. 0) then
  474.                               write(*,*) 'Invalid number of PE'
  475.                               write(*,*) 'Please check partitioning pattern'
  476.                               write(*,*) '                 or number of  PE'
  477.                            end if
  478.  +                         call mpi_finalize(ierr)
  479.                            stop
  480.                         end if
  481.                   C
  482.                         icomm= mpi_comm_world
  483.                   c
  484.                         idm(1)= ndx
  485.                         idm(2)= ndy
  486.                         idm(3)= ndz
  487.                   C
  488.                         ipd(1)= .false.
  489.                         ipd(2)= .false.
  490.                         ipd(3)= .false.
  491.                         ir= .false.
  492.                   C
  493.  +                      call mpi_cart_create(icomm,
  494.                        >                     ndims,
  495.                        >                     idm,
  496.                        >                     ipd,
  497.                        >                     ir,
  498.                        >                     mpi_comm_cart,
  499.                        >                     ierr)
  500.  +                      call mpi_cart_get(mpi_comm_cart,
  501.                        >                  ndims,
  502.                        >                  idm,
  503.                        >                  ipd,
  504.                        >                  iop,
  505.                        >                  ierr)
  506.                   c
  507.                   c
  508.                         if(ndz .gt. 1) then
  509.  +                         call mpi_cart_shift(mpi_comm_cart,
  510.                        >                       2,
  511.                        >                       1,
  512.                        >                       npz(1),
  513.                        >                       npz(2),
  514.                        >                       ierr)
  515.                         end if
  516.                   c
  517.                         if(ndy .gt. 1) then
  518.  +                         call mpi_cart_shift(mpi_comm_cart,
  519.                        >                       1,
  520.                        >                       1,
  521.                        >                       npy(1),
  522.                        >                       npy(2),
  523.                        >                       ierr)
  524.                         end if
  525.                   c
  526.                         if(ndx .gt. 1) then
  527.  +                         call mpi_cart_shift(mpi_comm_cart,
  528.                        >                       0,
  529.                        >                       1,
  530.                        >                       npx(1),
  531.                        >                       npx(2),
  532.                        >                       ierr)
  533.                         end if
  534.                   c
  535.                         return
  536.                         end

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 468 
  "mpi_init" (called from "INITCOMM") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 469 
  "mpi_comm_size" (called from "INITCOMM") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 470 
  "mpi_comm_rank" (called from "INITCOMM") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 478 
  "mpi_finalize" (called from "INITCOMM") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 493 
  "mpi_cart_create" (called from "INITCOMM") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 500 
  "mpi_cart_get" (called from "INITCOMM") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 509 
  "mpi_cart_shift" (called from "INITCOMM") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 518 
  "mpi_cart_shift" (called from "INITCOMM") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 527 
  "mpi_cart_shift" (called from "INITCOMM") was not inlined because the compiler was unable to locate the routine.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                          S o u r c e   L i s t i n g
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  537.                   c
  538.                   c
  539.                   c
  540.                         subroutine initmax(mx,my,mz,ks)
  541.                   c
  542.                         IMPLICIT REAL*4(a-h,o-z)
  543.                   c
  544.                         include 'param.h'
  545.                         include 'mpif.h'
  546.                   C
  547.                         integer  itmp,ks
  548.                         integer  mx1(0:ndx),my1(0:ndy),mz1(0:ndz)
  549.                         integer  mx2(0:ndx),my2(0:ndy),mz2(0:ndz)
  550.                   C
  551.                   CC    define imax, communication direction
  552.                         itmp= mx/ndx
  553.                         mx1(0)= 0
  554.  + 1------------<       do  i=1,ndx
  555.    1                       if(i .le. mod(mx,ndx)) then
  556.    1                          mx1(i)= mx1(i-1) + itmp + 1
  557.    1                       else
  558.    1                          mx1(i)= mx1(i-1) + itmp
  559.    1                       end if
  560.    1------------>       end do
  561.    w------------<       do i=0,ndx-1
  562.    w                       mx2(i)= mx1(i+1) - mx1(i)
  563.    w                       if(i .ne. 0)     mx2(i)= mx2(i) + 1
  564.    w                       if(i .ne. ndx-1) mx2(i)= mx2(i) + 1
  565.    w------------>       end do
  566.                   c
  567.                         itmp= my/ndy
  568.                         my1(0)= 0
  569.  + 1------------<       do  i=1,ndy
  570.    1                       if(i .le. mod(my,ndy)) then
  571.    1                          my1(i)= my1(i-1) + itmp + 1
  572.    1                       else
  573.    1                          my1(i)= my1(i-1) + itmp
  574.    1                       end if
  575.    1------------>       end do
  576.    w------------<       do i=0,ndy-1
  577.    w                       my2(i)= my1(i+1) - my1(i)
  578.    w                       if(i .ne. 0)      my2(i)= my2(i) + 1
  579.    w                       if(i .ne. ndy-1)  my2(i)= my2(i) + 1
  580.    w------------>       end do
  581.                   c
  582.                         itmp= mz/ndz
  583.                         mz1(0)= 0
  584.    w------------<       do  i=1,ndz
  585.    w                       if(i .le. mod(mz,ndz)) then
  586.    w                          mz1(i)= mz1(i-1) + itmp + 1
  587.    w                       else
  588.    w                          mz1(i)= mz1(i-1) + itmp
  589.    w                       end if
  590.    w------------>       end do
  591.    w------------<       do i=0,ndz-1
  592.    w                       mz2(i)= mz1(i+1) - mz1(i)
  593.    w                       if(i .ne. 0)      mz2(i)= mz2(i) + 1
  594.    w                       if(i .ne. ndz-1)  mz2(i)= mz2(i) + 1
  595.    w------------>       end do
  596.                   c
  597.                         imax= mx2(iop(1))
  598.                         jmax= my2(iop(2))
  599.                         kmax= mz2(iop(3))
  600.                   c
  601.                         if(iop(3) .eq. 0) then
  602.                            ks= mz1(iop(3))
  603.                         else
  604.                            ks= mz1(iop(3)) - 1
  605.                         end if
  606.                   c
  607.                   c     j-k plane  divied by i-direction
  608.                         if(ndx .gt. 1) then
  609.  +                         call mpi_type_vector(jmax*kmax,
  610.                        >                        1,
  611.                        >                        mimax,
  612.                        >                        mpi_real4,
  613.                        >                        jkvec,
  614.                        >                        ierr)
  615.  +                         call mpi_type_commit(jkvec,
  616.                        >                        ierr)
  617.                         end if
  618.                   c
  619.                   c     i-k plane  divied by j-direction
  620.                         if(ndy .gt. 1) then
  621.  +                         call mpi_type_vector(kmax,
  622.                        >                        imax,
  623.                        >                        mimax*mjmax,
  624.                        >                        mpi_real4,
  625.                        >                        ikvec,
  626.                        >                        ierr)
  627.  +                         call mpi_type_commit(ikvec,
  628.                        >                        ierr)
  629.                         end if
  630.                   c
  631.                   c     new vector k-direction
  632.                         if(ndz .gt. 1) then
  633.  +                         call mpi_type_vector(jmax,
  634.                        >                        imax,
  635.                        >                        mimax,
  636.                        >                        mpi_real4,
  637.                        >                        ijvec,
  638.                        >                        ierr)
  639.  +                         call mpi_type_commit(ijvec,
  640.                        >                        ierr)
  641.                         end if
  642.                   c
  643.                         return
  644.                         end

ftn-6254 ftn: VECTOR File = himenoBMTxpr.f, Line = 554 
  A loop starting at line 554 was not vectorized because a recurrence was found on "mx1" at line 556.

ftn-6008 ftn: SCALAR File = himenoBMTxpr.f, Line = 561 
  A loop starting at line 561 was unwound.

ftn-6254 ftn: VECTOR File = himenoBMTxpr.f, Line = 569 
  A loop starting at line 569 was not vectorized because a recurrence was found on "my1" at line 571.

ftn-6008 ftn: SCALAR File = himenoBMTxpr.f, Line = 576 
  A loop starting at line 576 was unwound.

ftn-6008 ftn: SCALAR File = himenoBMTxpr.f, Line = 584 
  A loop starting at line 584 was unwound.

ftn-6008 ftn: SCALAR File = himenoBMTxpr.f, Line = 591 
  A loop starting at line 591 was unwound.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 609 
  "mpi_type_vector" (called from "initmax") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 615 
  "mpi_type_commit" (called from "initmax") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 621 
  "mpi_type_vector" (called from "initmax") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 627 
  "mpi_type_commit" (called from "initmax") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 633 
  "mpi_type_vector" (called from "initmax") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 639 
  "mpi_type_commit" (called from "initmax") was not inlined because the compiler was unable to locate the routine.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                          S o u r c e   L i s t i n g
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  645.                   c
  646.                   c
  647.                   c
  648.                         subroutine sendp(mdx,mdy,mdz,
  649.                        & pack_p11,pack_p12,pack_p21,pack_p22,
  650.                        & pack_p31,pack_p32,unpack_p11,unpack_p12,
  651.                        & unpack_p21,unpack_p22,unpack_p31,unpack_p32)
  652.                         IMPLICIT REAL*4(a-h,o-z)
  653.                   c
  654.                         include 'param.h'
  655.                         real pack_p11(kmax*jmax),pack_p12(kmax*jmax)
  656.                         real pack_p21(kmax*imax),pack_p22(kmax*imax)
  657.                         real pack_p31(imax*jmax),pack_p32(imax*jmax)
  658.                         real unpack_p11(kmax*jmax),unpack_p12(kmax*jmax)
  659.                         real unpack_p21(kmax*imax),unpack_p22(kmax*imax)
  660.                         real unpack_p31(imax*jmax),unpack_p32(imax*jmax)
  661.                   C
  662.                         if(mdz .gt. 1) then
  663.  +                         call sendp3(
  664.                        & pack_p31,pack_p32,unpack_p31,unpack_p32)
  665.                         end if
  666.                   c
  667.                         if(mdy .gt. 1) then
  668.  +                         call sendp2(
  669.                        & pack_p21,pack_p22,unpack_p21,unpack_p22)
  670.                         end if
  671.                   c
  672.                         if(mdx .gt. 1) then
  673.  +                         call sendp1(
  674.                        & pack_p11,pack_p12,unpack_p11,unpack_p12)
  675.                         end if
  676.                   c
  677.                         return
  678.                         end

ftn-3118 ftn: IPA File = himenoBMTxpr.f, Line = 663 
  "sendp3" (called from "sendp") was not inlined because the call site will not flatten.  Routine "mpi_irecv" is missing.

ftn-3118 ftn: IPA File = himenoBMTxpr.f, Line = 668 
  "sendp2" (called from "sendp") was not inlined because the call site will not flatten.  Routine "mpi_irecv" is missing.

ftn-3118 ftn: IPA File = himenoBMTxpr.f, Line = 673 
  "sendp1" (called from "sendp") was not inlined because the call site will not flatten.  Routine "mpi_irecv" is missing.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                          S o u r c e   L i s t i n g
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  679.                   c
  680.                   c
  681.                   c
  682.                         subroutine sendp3(
  683.                        & pack_p31,pack_p32,unpack_p31,unpack_p32)
  684.                         IMPLICIT REAL*4(a-h,o-z)
  685.                   c
  686.                   c
  687.                         include 'mpif.h'
  688.                         include 'param.h'
  689.                   c
  690.                         real pack_p31(imax*jmax),pack_p32(imax*jmax)
  691.                         real unpack_p31(imax*jmax),unpack_p32(imax*jmax)
  692.                         dimension ist(mpi_status_size,0:3),ireq(0:3)
  693.                         data ireq /4*mpi_request_null/
  694.                   c
  695.  +                      call mpi_irecv(unpack_p32,
  696.                        >               imax*jmax,
  697.                        >               MPI_REAL,
  698.                        >               npz(2),
  699.                        >               1,
  700.                        >               mpi_comm_cart,
  701.                        >               ireq(3),
  702.                        >               ierr)
  703.                   c
  704.  +                      call mpi_irecv(unpack_p31,
  705.                        >               imax*jmax,
  706.                        >               MPI_REAL,
  707.                        >               npz(1),
  708.                        >               2,
  709.                        >               mpi_comm_cart,
  710.                        >               ireq(2),
  711.                        >               ierr)
  712.                   c
  713.  +                      call mpi_isend(pack_p31,
  714.                        >               imax*jmax,
  715.                        >               MPI_REAL,
  716.                        >               npz(1),
  717.                        >               1,
  718.                        >               mpi_comm_cart,
  719.                        >               ireq(0),
  720.                        >               ierr)
  721.                   c
  722.  +                      call mpi_isend(pack_p32,
  723.                        >               imax*jmax,
  724.                        >               MPI_REAL,
  725.                        >               npz(2),
  726.                        >               2,
  727.                        >               mpi_comm_cart,
  728.                        >               ireq(1),
  729.                        >               ierr)
  730.                   c
  731.  +                      call mpi_waitall(4,
  732.                        >                 ireq,
  733.                        >                 ist,
  734.                        >                 ierr)
  735.                   c
  736.                         return
  737.                         end

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 695 
  "mpi_irecv" (called from "sendp3") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 704 
  "mpi_irecv" (called from "sendp3") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 713 
  "mpi_isend" (called from "sendp3") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 722 
  "mpi_isend" (called from "sendp3") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 731 
  "mpi_waitall" (called from "sendp3") was not inlined because the compiler was unable to locate the routine.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                          S o u r c e   L i s t i n g
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  738.                   c
  739.                   c
  740.                   c
  741.                         subroutine sendp2(
  742.                        & pack_p21,pack_p22,unpack_p21,unpack_p22)
  743.                   c
  744.                         IMPLICIT REAL*4(a-h,o-z)
  745.                   c
  746.                         include 'mpif.h'
  747.                         include 'param.h'
  748.                   c
  749.                         real pack_p21(kmax*imax),pack_p22(kmax*imax)
  750.                         real unpack_p21(kmax*imax),unpack_p22(kmax*imax)
  751.                         dimension ist(mpi_status_size,0:3),ireq(0:3)
  752.                         data ireq /4*mpi_request_null/
  753.                   c
  754.  +                      call mpi_irecv(unpack_p21,
  755.                        >               kmax*imax,
  756.                        >               MPI_REAL,
  757.                        >               npy(1),
  758.                        >               2,
  759.                        >               mpi_comm_cart,
  760.                        >               ireq(3),
  761.                        >               ierr)
  762.                   c
  763.  +                      call mpi_irecv(unpack_p22,
  764.                        >               kmax*imax,
  765.                        >               MPI_REAL,
  766.                        >               npy(2),
  767.                        >               1,
  768.                        >               mpi_comm_cart,
  769.                        >               ireq(2),
  770.                        >               ierr)
  771.                   c
  772.  +                      call mpi_isend(pack_p21,
  773.                        >               kmax*imax,
  774.                        >               MPI_REAL,
  775.                        >               npy(1),
  776.                        >               1,
  777.                        >               mpi_comm_cart,
  778.                        >               ireq(0),
  779.                        >               ierr)
  780.                   c
  781.  +                      call mpi_isend(pack_p22,
  782.                        >               kmax*imax,
  783.                        >               MPI_REAL,
  784.                        >               npy(2),
  785.                        >               2,
  786.                        >               mpi_comm_cart,
  787.                        >               ireq(1),
  788.                        >               ierr)
  789.                   c
  790.  +                      call mpi_waitall(4,
  791.                        >                 ireq,
  792.                        >                 ist,
  793.                        >                 ierr)
  794.                   c
  795.                         return
  796.                         end

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 754 
  "mpi_irecv" (called from "sendp2") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 763 
  "mpi_irecv" (called from "sendp2") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 772 
  "mpi_isend" (called from "sendp2") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 781 
  "mpi_isend" (called from "sendp2") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 790 
  "mpi_waitall" (called from "sendp2") was not inlined because the compiler was unable to locate the routine.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                          S o u r c e   L i s t i n g
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  797.                   c
  798.                   c
  799.                   c
  800.                         subroutine sendp1(
  801.                        & pack_p11,pack_p12,unpack_p11,unpack_p12)
  802.                   c
  803.                         IMPLICIT REAL*4(a-h,o-z)
  804.                   c
  805.                         include 'mpif.h'
  806.                         include 'param.h'
  807.                   c
  808.                         real pack_p11(jmax*kmax),pack_p12(jmax*kmax)
  809.                         real unpack_p11(jmax*kmax),unpack_p12(jmax*kmax)
  810.                         dimension ist(mpi_status_size,0:3),ireq(0:3)
  811.                         data ireq /4*mpi_request_null/
  812.                   c
  813.  +                      call mpi_irecv(unpack_p11,
  814.                        >               jmax*kmax,
  815.                        >               MPI_REAL,
  816.                        >               npx(1),
  817.                        >               2,
  818.                        >               mpi_comm_cart,
  819.                        >               ireq(3),
  820.                        >               ierr)
  821.                   c
  822.  +                      call mpi_irecv(unpack_p12,
  823.                        >               jmax*kmax,
  824.                        >               MPI_REAL,
  825.                        >               npx(2),
  826.                        >               1,
  827.                        >               mpi_comm_cart,
  828.                        >               ireq(2),
  829.                        >               ierr)
  830.                   c
  831.  +                      call mpi_isend(pack_p11,
  832.                        >               jmax*kmax,
  833.                        >               MPI_REAL,
  834.                        >               npx(1),
  835.                        >               1,
  836.                        >               mpi_comm_cart,
  837.                        >               ireq(0),
  838.                        >               ierr)
  839.                   c
  840.  +                      call mpi_isend(pack_p12,
  841.                        >               jmax*kmax,
  842.                        >               MPI_REAL,
  843.                        >               npx(2),
  844.                        >               2,
  845.                        >               mpi_comm_cart,
  846.                        >               ireq(1),
  847.                        >               ierr)
  848.                   c
  849.  +                      call mpi_waitall(4,
  850.                        >                 ireq,
  851.                        >                 ist,
  852.                        >                 ierr)
  853.                   c
  854.                         return
  855.                         end

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 813 
  "mpi_irecv" (called from "sendp1") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 822 
  "mpi_irecv" (called from "sendp1") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 831 
  "mpi_isend" (called from "sendp1") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 840 
  "mpi_isend" (called from "sendp1") was not inlined because the compiler was unable to locate the routine.

ftn-3021 ftn: IPA File = himenoBMTxpr.f, Line = 849 
  "mpi_waitall" (called from "sendp1") was not inlined because the compiler was unable to locate the routine.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
